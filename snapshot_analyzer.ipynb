{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot find .env file\n"
     ]
    }
   ],
   "source": [
    "!pip install setuptools==67.2.0 -q\n",
    "!pip install --no-cache-dir mercury -q\n",
    "#To see the summary of cluster\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv\n",
    "    \n",
    "from docbooks.lib.sql import Cluster\n",
    "import pandas as pd\n",
    "c = Cluster.from_id(id=\"\")\n",
    "snap = c.snapshot\n",
    "snap.dump_to_file(\"cluster_snap.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>The above code should be executed, then allow some time for the snapshot JSON (cluster_snap.json) file to be generated.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Deployment Analyzer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import mercury as mr\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Helper function to match a deployment with PDB or HPA\n",
    "def get_resource_for_deployment(deployment, resources, resource_type):\n",
    "    pod_labels = deployment.get('spec', {}).get('template', {}).get('metadata', {}).get('labels', {})\n",
    "    name = deployment.get('metadata', {}).get('name', 'N/A')\n",
    "    namespace = deployment.get('metadata', {}).get('namespace', 'N/A')\n",
    "    if resources is None:\n",
    "        return 'N/A'\n",
    "\n",
    "    for resource in resources:\n",
    "        res_namespace = resource.get('metadata', {}).get('namespace', 'N/A')\n",
    "        if resource_type == 'pdb':\n",
    "            res_selector = resource.get('spec', {}).get('selector', {}).get('matchLabels', {})\n",
    "            if res_namespace == namespace:\n",
    "                # Check if all PDB labels are present in the deployment's labels\n",
    "                if all(item in pod_labels.items() for item in res_selector.items()):\n",
    "                    return resource\n",
    "        elif resource_type == 'hpa':\n",
    "            hpa_target = resource.get('spec', {}).get('scaleTargetRef', {})\n",
    "            hpa_name = hpa_target.get('name', 'N/A')\n",
    "            if hpa_name == name and res_namespace == namespace:\n",
    "                return resource\n",
    "    return 'N/A'\n",
    "\n",
    "# Function to get PVCs for a given deployment\n",
    "def get_pvcs_for_deployment(deployment, pvcs):\n",
    "    matched_pvcs = []\n",
    "    volume_claims = deployment.get('spec', {}).get('template', {}).get('spec', {}).get('volumes', [])\n",
    "\n",
    "    for claim in volume_claims:\n",
    "        claim_name = claim.get('persistentVolumeClaim', {}).get('claimName', 'N/A')\n",
    "        matched_pvcs.extend(pvc for pvc in pvcs if pvc.get('metadata', {}).get('name') == claim_name \n",
    "                            and pvc.get('metadata', {}).get('namespace') == deployment.get('metadata', {}).get('namespace'))\n",
    "    return matched_pvcs\n",
    "\n",
    "# Function to extract data and save it as JSON, and return a DataFrame and JSON structure\n",
    "def extract_and_save(json_data):\n",
    "    try:\n",
    "        data = json.loads(json_data)\n",
    "    \n",
    "        # Extracting data from JSON\n",
    "        deployments = data.get('deploymentList', {}).get('items', [])\n",
    "        if not isinstance(deployments, list):\n",
    "            return pd.DataFrame(), []\n",
    "        resources = {\n",
    "            'pdbs': data.get('podDisruptionBudgetList', {}).get('items', []),\n",
    "            'hpas': data.get('horizontalPodAutoscalerList', {}).get('items', []),\n",
    "            'pvcs': data.get('persistentVolumeClaimList', {}).get('items', []),\n",
    "            'pvs': data.get('persistentVolumeList', {}).get('items', [])\n",
    "        }\n",
    "    \n",
    "        csv_columns = [\n",
    "            'Deployment Name', 'Namespace', 'Affinity', 'Topology Spread Constraints',\n",
    "            'Node Selector', 'Tolerations', \n",
    "            'PDB Spec', \n",
    "            'HPA Spec', 'PVC', 'PV'\n",
    "        ]\n",
    "    \n",
    "        csv_data = []\n",
    "        json_data_list = []\n",
    "    \n",
    "        for deployment in deployments:\n",
    "            name = deployment.get('metadata', {}).get('name', 'N/A')\n",
    "            namespace = deployment.get('metadata', {}).get('namespace', 'N/A')\n",
    "            spec = deployment.get('spec', {}).get('template', {}).get('spec', {})\n",
    "            \n",
    "            pdb = get_resource_for_deployment(deployment, resources['pdbs'], 'pdb')\n",
    "            hpa = get_resource_for_deployment(deployment, resources['hpas'], 'hpa')\n",
    "            matched_pvcs = get_pvcs_for_deployment(deployment, resources['pvcs'])\n",
    "            # Update this part to consider namespace when matching PVs\n",
    "            matched_pvs = []\n",
    "            for pvc in matched_pvcs:\n",
    "                pvc_name = pvc.get('metadata', {}).get('name')\n",
    "                pvc_namespace = pvc.get('metadata', {}).get('namespace')\n",
    "                # Find matching PVs that correspond to the PVC and are in the same namespace\n",
    "                matched_pvs.extend(pv for pv in resources['pvs'] if\n",
    "                                   pv.get('spec', {}).get('claimRef', {}).get('name') == pvc_name and\n",
    "                                   pv.get('spec', {}).get('claimRef', {}).get('namespace') == pvc_namespace)\n",
    "\n",
    "            \n",
    "            row_data = {\n",
    "                'Deployment Name': name,\n",
    "                'Namespace': namespace,\n",
    "                'Affinity': spec.get('affinity', {}),\n",
    "                'Topology Spread Constraints': spec.get('topologySpreadConstraints', []),\n",
    "                'Node Selector': spec.get('nodeSelector', {}),\n",
    "                'Tolerations': spec.get('tolerations', []),\n",
    "                'PDB Spec': pdb.get('spec') if pdb != 'N/A' else 'N/A',\n",
    "                'HPA Spec': hpa.get('spec') if hpa != 'N/A' else 'N/A',\n",
    "                'PVC': matched_pvcs,\n",
    "                'PV': matched_pvs\n",
    "            }\n",
    "            csv_data.append(list(row_data.values()))\n",
    "            json_data_list.append(row_data)\n",
    "        \n",
    "        # Convert to DataFrame and JSON format\n",
    "        df = pd.DataFrame(csv_data, columns=csv_columns)\n",
    "        return df, json_data_list\n",
    "\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame(), []  # Return an empty DataFrame and JSON list on error\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    try:\n",
    "        with open('cluster_snap.json', 'r') as json_file:\n",
    "            json_data = json_file.read()\n",
    "        df, json_data_list = extract_and_save(json_data)\n",
    "        \n",
    "        mr.Table(df)\n",
    "        #mr.JSON(json_data_list)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>StatefulSet Analyzer (Configurations)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import mercury as mr\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Helper function to match a stateful set with PDB or HPA\n",
    "def get_resource_for_statefulset(statefulset, resources, resource_type):\n",
    "    pod_labels = statefulset.get('spec', {}).get('template', {}).get('metadata', {}).get('labels', {})\n",
    "    name = statefulset.get('metadata', {}).get('name', 'N/A')\n",
    "    namespace = statefulset.get('metadata', {}).get('namespace', 'N/A')\n",
    "    if resources is None:\n",
    "        return 'N/A'\n",
    "\n",
    "    for resource in resources:\n",
    "        res_namespace = resource.get('metadata', {}).get('namespace', 'N/A')\n",
    "        if resource_type == 'pdb':\n",
    "            res_selector = resource.get('spec', {}).get('selector', {}).get('matchLabels', {})\n",
    "            if res_namespace == namespace:\n",
    "                # Check if all PDB labels are present in the deployment's labels\n",
    "                if all(item in pod_labels.items() for item in res_selector.items()):\n",
    "                    return resource\n",
    "        elif resource_type == 'hpa':\n",
    "            hpa_target = resource.get('spec', {}).get('scaleTargetRef', {})\n",
    "            hpa_name = hpa_target.get('name', 'N/A')\n",
    "            if hpa_name == name and res_namespace == namespace:\n",
    "                return resource\n",
    "    return 'N/A'\n",
    "\n",
    "# Function to extract PVCs for a given stateful set\n",
    "def get_volume_claim_templates(statefulset):\n",
    "    return statefulset.get('spec', {}).get('volumeClaimTemplates', [])\n",
    "\n",
    "# Function to extract data and save as JSON, returning DataFrame and JSON structure\n",
    "def extract_statefulset_data(json_data):\n",
    "    try:\n",
    "        data = json.loads(json_data)\n",
    "    \n",
    "        # Extracting data from JSON\n",
    "        statefulsets = data.get('statefulSetList', {}).get('items', [])\n",
    "        if not isinstance(statefulsets, list):\n",
    "            return pd.DataFrame(), []\n",
    "        resources = {\n",
    "            'pdbs': data.get('podDisruptionBudgetList', {}).get('items', []),\n",
    "            'hpas': data.get('horizontalPodAutoscalerList', {}).get('items', [])\n",
    "        }\n",
    "    \n",
    "        csv_columns = [\n",
    "            'StatefulSet Name', 'Namespace', 'Affinity', 'Topology Spread Constraints',\n",
    "            'Node Selector', 'Tolerations', 'Volume Claim Templates', 'Service Name',\n",
    "            'PDB Spec', 'HPA Spec'\n",
    "        ]\n",
    "    \n",
    "        csv_data = []\n",
    "        json_data_list = []\n",
    "    \n",
    "        for statefulset in statefulsets:\n",
    "            name = statefulset.get('metadata', {}).get('name', 'N/A')\n",
    "            namespace = statefulset.get('metadata', {}).get('namespace', 'N/A')\n",
    "            spec = statefulset.get('spec', {}).get('template', {}).get('spec', {})\n",
    "            \n",
    "            pdb = get_resource_for_statefulset(statefulset, resources['pdbs'], 'pdb')\n",
    "            hpa = get_resource_for_statefulset(statefulset, resources['hpas'], 'hpa')\n",
    "            volume_claim_templates = get_volume_claim_templates(statefulset)\n",
    "            \n",
    "            row_data = {\n",
    "                'StatefulSet Name': name,\n",
    "                'Namespace': namespace,\n",
    "                'Affinity': spec.get('affinity', {}),\n",
    "                'Topology Spread Constraints': spec.get('topologySpreadConstraints', []),\n",
    "                'Node Selector': spec.get('nodeSelector', {}),\n",
    "                'Tolerations': spec.get('tolerations', []),\n",
    "                'Volume Claim Templates': volume_claim_templates,\n",
    "                'Service Name': statefulset.get('spec', {}).get('serviceName', 'N/A'),\n",
    "                'PDB Spec': pdb.get('spec') if pdb != 'N/A' else 'N/A',\n",
    "                'HPA Spec': hpa.get('spec') if hpa != 'N/A' else 'N/A'\n",
    "            }\n",
    "            csv_data.append(list(row_data.values()))\n",
    "            json_data_list.append(row_data)\n",
    "        \n",
    "        # Convert to DataFrame and JSON format\n",
    "        df = pd.DataFrame(csv_data, columns=csv_columns)\n",
    "        return df, json_data_list\n",
    "\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame(), []  # Return an empty DataFrame and JSON list on error\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    try:\n",
    "        with open('cluster_snap.json', 'r') as json_file:\n",
    "            json_data = json_file.read()\n",
    "        df, json_data_list = extract_statefulset_data(json_data)\n",
    "        mr.Table(df)\n",
    "        #mr.JSON(json_data_list)\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>StatefulSet Analyzer (Storage Details)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import mercury as mr\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# Helper function to match PVCs for a pod\n",
    "def get_pvc_for_pod(pod, all_pvcs):\n",
    "    pod_namespace = pod.get('metadata', {}).get('namespace', 'N/A')  # Get the pod's namespace\n",
    "    return [pvc for claim in pod.get('spec', {}).get('volumes', []) if 'persistentVolumeClaim' in claim\n",
    "            for pvc in all_pvcs \n",
    "            if (pvc.get('metadata', {}).get('name') == claim['persistentVolumeClaim'].get('claimName', 'N/A') and \n",
    "                pvc.get('metadata', {}).get('namespace') == pod_namespace)]  # Check namespace\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to match PVs for a PVC\n",
    "def get_pv_for_pvc(pvc, pvs):\n",
    "    pvc_namespace = pvc.get('metadata', {}).get('namespace', 'N/A')  # Get the PVC's namespace\n",
    "    pvc_name = pvc.get('metadata', {}).get('name', 'N/A')  # Get the PVC's name\n",
    "    return [pv for pv in pvs if pv.get('spec', {}).get('claimRef', {}).get('name') == pvc_name and\n",
    "            pv.get('spec', {}).get('claimRef', {}).get('namespace') == pvc_namespace]  # Check namespace\n",
    "\n",
    "# Function to extract StatefulSet, PVC, and PV data\n",
    "def extract_statefulset_storage_data(json_data):\n",
    "    try:\n",
    "        data = json.loads(json_data)\n",
    "        statefulsets = data.get('statefulSetList', {}).get('items', [])\n",
    "        if not isinstance(statefulsets, list):\n",
    "            return pd.DataFrame(), []\n",
    "        \n",
    "        pvcs = data.get('persistentVolumeClaimList', {}).get('items', [])\n",
    "        pvs = data.get('persistentVolumeList', {}).get('items', [])\n",
    "        pod_list = data.get('podList', {}).get('items', [])\n",
    "\n",
    "        csv_columns = ['Pod Name', 'StatefulSet Name', 'Namespace', 'PVC', 'PV']\n",
    "        csv_data = []\n",
    "        json_data_output = []\n",
    "\n",
    "        for statefulset in statefulsets:\n",
    "            statefulset_name = statefulset.get('metadata', {}).get('name', 'N/A')\n",
    "            namespace = statefulset.get('metadata', {}).get('namespace', 'N/A')\n",
    "\n",
    "            for pod in pod_list:\n",
    "                pod_name = pod.get('metadata', {}).get('name', 'N/A')\n",
    "                if pod_name.startswith(statefulset_name):\n",
    "                    matched_pvcs = get_pvc_for_pod(pod, pvcs)\n",
    "                    matched_pvs = [get_pv_for_pvc(pvc, pvs) for pvc in matched_pvcs]  # Get PVs for each matched PVC\n",
    "                    matched_pvs_flat = [pv for sublist in matched_pvs for pv in sublist]  # Flatten the list of lists\n",
    "\n",
    "                    row_data = {\n",
    "                        'Pod Name': pod_name,\n",
    "                        'StatefulSet Name': statefulset_name,\n",
    "                        'Namespace': namespace,\n",
    "                        'PVC': matched_pvcs,\n",
    "                        'PV': matched_pvs_flat  # Use the flattened list of PVs\n",
    "                    }\n",
    "                    csv_data.append(list(row_data.values()))\n",
    "                    json_data_output.append(row_data)\n",
    "\n",
    "        # Convert to DataFrame and JSON format\n",
    "        df = pd.DataFrame(csv_data, columns=csv_columns)\n",
    "        return df, json_data_output\n",
    "\n",
    "    except (json.JSONDecodeError, FileNotFoundError) as e:\n",
    "        logging.error(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame(), []  # Return an empty DataFrame and JSON list on error\n",
    "\n",
    "# Main function to execute the script\n",
    "def main():\n",
    "    try:\n",
    "        with open('cluster_snap.json', 'r') as json_file:\n",
    "            json_data = json_file.read()\n",
    "        df, json_data_output = extract_statefulset_storage_data(json_data)\n",
    "        mr.Table(df)\n",
    "        #mr.JSON(json_data_output)\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"File not found: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pod Details in the cluster, grouped by owner reference.</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import mercury as mr\n",
    "\n",
    "# Initialize a defaultdict to group pods by owner reference, tracking pods, nodes, and other configurations\n",
    "owner_reference_groups = defaultdict(lambda: {\n",
    "    \"Pods\": [], \"Nodes\": set(), \"ControllerKind\": \"\", \"Affinity\": None, \n",
    "    \"NodeSelector\": None, \"TopologySpreadConstraints\": None, \"Tolerations\": None\n",
    "})\n",
    "\n",
    "# Iterate through the pods and group them by owner reference, excluding DaemonSet pods\n",
    "for p in snap.pods:\n",
    "    if p.controller_kind != \"DaemonSet\":\n",
    "        owner_name = p.controller if p.controller else 'Unknown'\n",
    "        owner_reference_groups[owner_name][\"Pods\"].append(p.name)\n",
    "        owner_reference_groups[owner_name][\"Nodes\"].add(p.node_name)  # Track the node the pod is scheduled on\n",
    "        owner_reference_groups[owner_name][\"Affinity\"] = p.affinity  # Capture affinity\n",
    "        owner_reference_groups[owner_name][\"NodeSelector\"] = p.node_selector  # Capture nodeSelector\n",
    "        # Extract topologySpreadConstraints from the pod's spec using get()\n",
    "        owner_reference_groups[owner_name][\"TopologySpreadConstraints\"] = p.spec.get('topologySpreadConstraints', None)\n",
    "        owner_reference_groups[owner_name][\"Tolerations\"] = p.tolerations  # Capture tolerations\n",
    "        owner_reference_groups[owner_name][\"ControllerKind\"] = p.controller_kind  # Set controller kind\n",
    "\n",
    "# Prepare data for DataFrame, ensuring ControllerKind is the second column\n",
    "pod_data = [{\n",
    "    \"OwnerReference\": owner, \n",
    "    \"ControllerKind\": data[\"ControllerKind\"],  # Make ControllerKind the second column\n",
    "    \"PodCount\": len(data[\"Pods\"]), \n",
    "    \"NodeCount\": len(data[\"Nodes\"]),  # Count the number of unique nodes\n",
    "    \"Affinity\": data[\"Affinity\"],  # Affinity (same for all pods under a single owner reference)\n",
    "    \"NodeSelector\": data[\"NodeSelector\"],  # NodeSelector (same for all pods under a single owner reference)\n",
    "    \"TopologySpreadConstraints\": data[\"TopologySpreadConstraints\"],  # Extracted from the pod spec\n",
    "    \"Tolerations\": data[\"Tolerations\"]  # Tolerations\n",
    "} for owner, data in owner_reference_groups.items()]\n",
    "\n",
    "# Create a DataFrame\n",
    "pod_df = pd.DataFrame(pod_data)\n",
    "\n",
    "# Display the DataFrame using mercury\n",
    "mr.Table(pod_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a571c2d0a6f8eeaffd971d243bebf18b128ebc53b75e81fbb0c747ffdac0cb03"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
